{
  "timestamp": "2025-07-29T16:21:54.403875",
  "documentation": {
    "CONTEXT.md": "# Project Context\n\n## Overview\nThe CFO Forecast Tool is an AI-powered financial forecasting system that automates cash flow projections for small businesses. It transforms manual spreadsheet-based forecasting into an automated system using pattern recognition and machine learning.\n\n## Current State\n- **Phase**: Initial Refactoring\n- **Last Updated**: 2025-01-29\n- **Active Client**: spyguy (hardcoded)\n- **Data Source**: Mercury CSV exports only\n\n## Architecture\n\n### Core Components\n1. **Data Import Layer**\n   - `import_mercury_csv.py`: Handles Mercury bank CSV imports\n   - Transaction deduplication logic\n   - Data validation and normalization\n\n2. **AI/ML Layer**\n   - `ai_group_vendors.py`: OpenAI embeddings for vendor clustering\n   - `openai_infer.py`: Vendor name normalization using GPT-4\n   - Pattern detection algorithms\n\n3. **Forecasting Engine**\n   - `vendor_forecast.py`: Core forecasting logic\n   - `run_forecast.py`: Main forecasting pipeline\n   - Multiple forecasting methods (regular, irregular, manual)\n\n4. **UI Layer**\n   - `mapping_review.py`: Streamlit dashboard for vendor management\n   - `forecast_overview.py`: Forecast visualization dashboard\n   - `variance_review.py`: Variance analysis interface\n\n5. **Data Layer**\n   - Supabase (PostgreSQL) backend\n   - Tables: clients, transactions, vendors, forecasts\n\n### Tech Stack\n- **Backend**: Python 3.8+\n- **Database**: Supabase (PostgreSQL)\n- **AI/ML**: OpenAI API (GPT-4, embeddings)\n- **UI**: Streamlit\n- **Data Processing**: Pandas, NumPy, scikit-learn\n\n## Key Business Logic\n\n### Vendor Classification\n- **Regular**: Consistent frequency (weekly, bi-weekly, monthly)\n- **Quasi-Regular**: Somewhat predictable patterns\n- **Irregular**: No clear pattern, requires manual forecasting\n\n### Forecasting Methods\n1. **Pattern-Based**: For regular vendors with clear frequencies\n2. **Average-Based**: For irregular vendors using historical averages\n3. **Manual Override**: For special cases or known future changes\n\n### Confidence Scoring\n- Based on transaction history consistency\n- Pattern detection accuracy\n- Number of historical data points\n\n## Current Limitations\n1. Single client hardcoded (\"spyguy\")\n2. Only Mercury CSV imports supported\n3. No credit card integration\n4. No inventory planning integration\n5. Sequential processing (no parallelization)\n6. No caching layer\n\n## Maintenance Instructions\nUpdate this file when:\n- Major architectural decisions are made\n- New components are added\n- Business logic changes\n- Integration points change\n- Key limitations are addressed",
    "SETUP.md": "# Development Setup Guide\n\n## Prerequisites\n\n### Required Software\n- Python 3.8 or higher\n- pip (Python package manager)\n- Git\n- A Supabase account\n- An OpenAI API account\n\n### Recommended Software\n- pyenv (for Python version management)\n- virtualenv or venv\n- VS Code or PyCharm\n- PostgreSQL client (for direct database access)\n\n## Initial Setup\n\n### 1. Clone the Repository\n```bash\ngit clone <repository-url>\ncd cfo_forecast_refactored\n```\n\n### 2. Set Up Python Environment\n```bash\n# Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\n# On macOS/Linux:\nsource venv/bin/activate\n# On Windows:\nvenv\\Scripts\\activate\n\n# Upgrade pip\npip install --upgrade pip\n```\n\n### 3. Install Dependencies\n```bash\n# Install all requirements\npip install -r requirements.txt\n\n# If requirements.txt doesn't exist, install manually:\npip install streamlit pandas numpy scikit-learn supabase openai python-dotenv\n```\n\n### 4. Environment Configuration\n```bash\n# Create .env file from example\ncp .env.example .env\n\n# Edit .env with your credentials\n```\n\nYour `.env` file should contain:\n```\n# Supabase Configuration\nSUPABASE_URL=https://your-project.supabase.co\nSUPABASE_KEY=your-anon-key\n\n# OpenAI Configuration\nOPENAI_API_KEY=sk-your-api-key\n\n# Application Configuration\nDEFAULT_CLIENT_ID=spyguy\nLOG_LEVEL=INFO\n```\n\n### 5. Database Setup\n\n#### Option A: Using Existing Supabase Project\n1. Get your Supabase URL and anon key from project settings\n2. Add them to your `.env` file\n3. Ensure you have the required tables (see Database Schema below)\n\n#### Option B: Creating New Supabase Project\n1. Create new project at https://supabase.com\n2. Run the SQL migrations in `migrations/` directory (if available)\n3. Or create tables manually using the schema below\n\n### 6. Verify Setup\n```bash\n# Test Supabase connection\npython -c \"from supabase_client import supabase; print('Connected!' if supabase else 'Failed!')\"\n\n# Test OpenAI connection\npython -c \"from openai_client import openai_client; print('Connected!' if openai_client else 'Failed!')\"\n```\n\n## Database Schema\n\n### Required Tables\n\n```sql\n-- Clients table\nCREATE TABLE clients (\n    id TEXT PRIMARY KEY,\n    name TEXT NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Vendors table\nCREATE TABLE vendors (\n    id SERIAL PRIMARY KEY,\n    client_id TEXT REFERENCES clients(id),\n    vendor_name TEXT NOT NULL,\n    display_name TEXT,\n    category TEXT,\n    vendor_group TEXT,\n    group_locked BOOLEAN DEFAULT FALSE,\n    review_needed BOOLEAN DEFAULT FALSE,\n    forecast_method TEXT,\n    forecast_frequency TEXT,\n    forecast_day TEXT,\n    forecast_amount DECIMAL(10,2),\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Transactions table\nCREATE TABLE transactions (\n    id SERIAL PRIMARY KEY,\n    client_id TEXT REFERENCES clients(id),\n    transaction_date DATE NOT NULL,\n    vendor_name TEXT NOT NULL,\n    amount DECIMAL(10,2) NOT NULL,\n    description TEXT,\n    category TEXT,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Forecasts table\nCREATE TABLE forecasts (\n    id SERIAL PRIMARY KEY,\n    client_id TEXT REFERENCES clients(id),\n    forecast_date DATE NOT NULL,\n    vendor_name TEXT,\n    amount DECIMAL(10,2),\n    forecast_method TEXT,\n    confidence_score DECIMAL(3,2),\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Create indexes for performance\nCREATE INDEX idx_vendors_client_display ON vendors(client_id, display_name);\nCREATE INDEX idx_transactions_client_date ON transactions(client_id, transaction_date);\nCREATE INDEX idx_transactions_vendor ON transactions(client_id, vendor_name);\n```\n\n## Running the Application\n\n### 1. Import Mercury CSV Data\n```bash\n# Update the file path and client_id in import_mercury_csv.py\npython import_mercury_csv.py\n```\n\n### 2. Run AI Vendor Grouping\n```bash\npython ai_group_vendors.py\n```\n\n### 3. Generate Forecasts\n```bash\npython run_forecast.py\n```\n\n### 4. Launch Dashboard\n```bash\n# For vendor mapping review\nstreamlit run mapping_review.py\n\n# Or use the launcher\npython run_dashboard.py\n```\n\n## Development Workflow\n\n### 1. Before Starting Work\n```bash\n# Activate virtual environment\nsource venv/bin/activate\n\n# Pull latest changes\ngit pull origin main\n\n# Check documentation for context\npython main.py --context\n```\n\n### 2. During Development\n```bash\n# Add todos for your tasks\npython main.py --add-todo \"Implement new feature\"\n\n# Update progress regularly\npython utils/progress_tracker.py update \"Completed vendor grouping optimization\"\n\n# Check current status\npython main.py --status\n```\n\n### 3. Before Committing\n```bash\n# Create snapshot of current state\npython main.py --snapshot\n\n# Run any tests (when implemented)\npytest\n\n# Update documentation\n# Manually update relevant files in docs/\n```\n\n## Troubleshooting\n\n### Common Issues\n\n#### 1. Supabase Connection Errors\n```\nError: Invalid API key\n```\n**Solution**: Check your SUPABASE_KEY in .env file\n\n#### 2. OpenAI Rate Limits\n```\nError: Rate limit exceeded\n```\n**Solution**: Wait and retry, or implement caching\n\n#### 3. Module Import Errors\n```\nError: No module named 'supabase_client'\n```\n**Solution**: Ensure you're in the project root directory\n\n#### 4. Streamlit Port Already in Use\n```\nError: Port 8501 is already in use\n```\n**Solution**: Kill the process or use different port:\n```bash\nstreamlit run app.py --server.port 8502\n```\n\n### Debug Mode\nEnable detailed logging by setting in .env:\n```\nLOG_LEVEL=DEBUG\n```\n\n## IDE Configuration\n\n### VS Code\nRecommended extensions:\n- Python\n- Pylance\n- Python Docstring Generator\n- GitLens\n\nSettings.json:\n```json\n{\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"python.formatting.provider\": \"black\",\n    \"editor.formatOnSave\": true\n}\n```\n\n### PyCharm\n1. Set project interpreter to your virtual environment\n2. Mark project root as Sources Root\n3. Enable code inspections\n\n## Additional Resources\n\n- [Supabase Documentation](https://supabase.com/docs)\n- [Streamlit Documentation](https://docs.streamlit.io)\n- [OpenAI API Reference](https://platform.openai.com/docs)\n- [Project Repository Wiki](https://github.com/your-repo/wiki)\n\n## Support\n\nFor issues or questions:\n1. Check ISSUES.md for known problems\n2. Review logs in the console\n3. Create detailed bug report with reproduction steps",
    "PROGRESS.md": "# Development Progress Log\n\n## Instructions\n- Log all significant development activities\n- Use ISO timestamp format (YYYY-MM-DD HH:MM)\n- Include phase, task description, and outcome\n- Mark milestones with \u2b50\n\n## Progress Entries\n\n### 2025-01-29 - Documentation System Implementation\n**Phase**: Infrastructure Setup  \n**Developer**: Assistant  \n**Tasks Completed**:\n- \u2705 Created comprehensive documentation structure in docs/\n- \u2705 Established tracking templates for all documentation files\n- \u2705 Set up progress tracking system\n\n**Notes**: Implementing documentation-as-infrastructure approach before any feature development\n\n---\n\n### 2025-01-28 - Initial Project Analysis\n**Phase**: Discovery  \n**Developer**: Assistant  \n**Tasks Completed**:\n- \u2705 Reviewed entire codebase structure\n- \u2705 Identified core components and architecture\n- \u2705 Analyzed strengths and weaknesses\n- \u2705 Created comprehensive project documentation\n\n**Key Findings**:\n- 40+ files in root directory need organization\n- Strong AI integration with OpenAI\n- Sophisticated forecasting logic\n- Hardcoded client configuration throughout\n\n---\n\n## Milestone Tracking\n\n### \u2b50 Milestones Achieved\n- [2025-01-28] Project analysis and documentation completed\n\n### \ud83c\udfaf Upcoming Milestones\n- [ ] Documentation system fully implemented\n- [ ] Progress tracker module operational\n- [ ] CLI integration complete\n- [ ] First refactoring phase begun\n\n## Template for New Entries\n\n```markdown\n### YYYY-MM-DD HH:MM - [Task Title]\n**Phase**: [Infrastructure/Refactoring/Feature/Bug Fix/Testing]  \n**Developer**: [Name]  \n**Tasks Completed**:\n- \u2705 [Specific task 1]\n- \u2705 [Specific task 2]\n- \u274c [Failed task with reason]\n\n**Code Changes**:\n- Modified: [file1.py] - [brief description]\n- Added: [file2.py] - [brief description]\n- Removed: [file3.py] - [brief description]\n\n**Notes**: [Any additional context, decisions made, or issues encountered]\n\n**Next Steps**: [What should be done next]\n```",
    "TODO.md": "# Project TODO List\n\n## Instructions\n- Organize tasks by phase and priority\n- Use checkbox format for easy tracking\n- Include estimated effort (S/M/L/XL)\n- Update status as tasks progress\n\n## Priority Levels\n- \ud83d\udd34 **Critical**: Blocking issues or security concerns\n- \ud83d\udfe1 **High**: Important for next release\n- \ud83d\udfe2 **Medium**: Should be done soon\n- \ud83d\udd35 **Low**: Nice to have\n\n## Phase 1: Foundation & Architecture (Current)\n\n### \ud83d\udd34 Critical\n- [x] Implement documentation system (L)\n- [ ] Create utils/progress_tracker.py module (M)\n- [ ] Add CLI commands for documentation management (M)\n- [ ] Refactor file structure into proper modules (XL)\n  - [ ] Create src/ directory structure\n  - [ ] Move files to appropriate subdirectories\n  - [ ] Update all imports\n- [ ] Remove hardcoded client_id throughout codebase (L)\n- [ ] Create centralized configuration system (M)\n\n### \ud83d\udfe1 High Priority\n- [ ] Implement proper error handling across all modules (L)\n- [ ] Add logging configuration (S)\n- [ ] Create requirements.txt with pinned versions (S)\n- [ ] Add .env.example file (S)\n- [ ] Implement connection pooling for Supabase (M)\n- [ ] Test the comprehensive documentation system (Added: 2025-07-29)\n\n### \ud83d\udfe2 Medium Priority\n- [ ] Add input validation for all user inputs (M)\n- [ ] Create data validation schemas (M)\n- [ ] Implement retry logic for API calls (M)\n- [ ] Add database migration system (L)\n\n## Phase 2: Testing & Quality\n\n### \ud83d\udfe1 High Priority\n- [ ] Add unit tests for core modules (XL)\n  - [ ] Test vendor classification logic\n  - [ ] Test forecasting algorithms\n  - [ ] Test data import functionality\n- [ ] Add integration tests (L)\n- [ ] Set up CI/CD pipeline (M)\n- [ ] Add code coverage reporting (S)\n\n### \ud83d\udfe2 Medium Priority\n- [ ] Add performance tests (M)\n- [ ] Implement load testing (M)\n- [ ] Add security scanning (M)\n\n## Phase 3: Feature Enhancement\n\n### \ud83d\udfe1 High Priority\n- [ ] Add credit card statement import (L)\n- [ ] Implement caching layer with Redis (L)\n- [ ] Add batch processing for vendors (M)\n- [ ] Implement background job processing (L)\n\n### \ud83d\udfe2 Medium Priority\n- [ ] Add inventory planning integration (XL)\n- [ ] Implement ML-based pattern improvement (XL)\n- [ ] Add seasonal adjustment algorithms (L)\n- [ ] Create API endpoints for external access (L)\n\n### \ud83d\udd35 Low Priority\n- [ ] Add mobile-responsive dashboard design (M)\n- [ ] Implement user preference management (M)\n- [ ] Add export functionality for reports (M)\n- [ ] Create onboarding wizard for new clients (L)\n\n## Phase 4: Enterprise Features\n\n### \ud83d\udfe2 Medium Priority\n- [ ] Implement multi-user support with RBAC (XL)\n- [ ] Add audit logging system (L)\n- [ ] Create admin dashboard (L)\n- [ ] Add team collaboration features (L)\n\n### \ud83d\udd35 Low Priority\n- [ ] Add white-label customization (M)\n- [ ] Implement SSO integration (L)\n- [ ] Add advanced analytics dashboard (XL)\n- [ ] Create mobile app (XL)\n\n## Bug Fixes & Issues\n\n### \ud83d\udd34 Critical\n- [ ] Fix potential SQL injection vulnerabilities (M)\n- [ ] Address missing error handling in API calls (S)\n\n### \ud83d\udfe1 High Priority\n- [ ] Fix dashboard loading performance (M)\n- [ ] Resolve duplicate transaction edge cases (M)\n\n## Technical Debt\n\n### \ud83d\udfe1 High Priority\n- [ ] Refactor monolithic files into smaller modules (L)\n- [ ] Remove code duplication across modules (M)\n- [ ] Standardize naming conventions (M)\n- [ ] Add type hints throughout codebase (L)\n\n### \ud83d\udfe2 Medium Priority\n- [ ] Optimize database queries (M)\n- [ ] Implement proper transaction handling (M)\n- [ ] Add database indexes for performance (S)\n\n## Template for New Tasks\n\n```markdown\n- [ ] [Task description] (Effort: S/M/L/XL)\n  - Additional context or subtasks\n  - Dependencies: [what must be done first]\n  - Assignee: [who will do this]\n```\n\n## Effort Sizing Guide\n- **S (Small)**: < 2 hours\n- **M (Medium)**: 2-8 hours\n- **L (Large)**: 1-3 days\n- **XL (Extra Large)**: 3+ days",
    "ISSUES.md": "# Known Issues & Blockers\n\n## Instructions\n- Document all bugs, blockers, and problems\n- Include reproduction steps when possible\n- Track attempted solutions\n- Update status as issues are resolved\n\n## Issue Template\n```markdown\n### [ISSUE-XXX] - [Issue Title]\n**Status**: [Open/In Progress/Blocked/Resolved]\n**Severity**: [Critical/High/Medium/Low]\n**Type**: [Bug/Performance/Security/Design Flaw]\n**Reported**: [Date]\n**Reporter**: [Who found it]\n\n**Description**:\n[Detailed description of the issue]\n\n**Reproduction Steps**:\n1. [Step 1]\n2. [Step 2]\n3. [Expected vs Actual result]\n\n**Error Messages/Logs**:\n```\n[Paste relevant errors or logs]\n```\n\n**Attempted Solutions**:\n1. [What was tried] - [Result]\n2. [What was tried] - [Result]\n\n**Proposed Solution**:\n[How to fix it]\n\n**Workaround**:\n[Temporary solution if available]\n\n**Related Files**:\n- [file1.py]\n- [file2.py]\n```\n\n## Current Issues\n\n### [ISSUE-001] - Hardcoded Client ID Throughout Codebase\n**Status**: Open  \n**Severity**: High  \n**Type**: Design Flaw  \n**Reported**: 2025-01-28  \n**Reporter**: Code Review  \n\n**Description**:\nThe client ID \"spyguy\" is hardcoded in multiple files, making it impossible to use the system for multiple clients without code changes.\n\n**Affected Files**:\n- run_forecast.py (lines 23, 34, 88)\n- ai_group_vendors.py (line 64)\n- vendor_forecast.py (multiple locations)\n- Most other Python files\n\n**Attempted Solutions**:\n1. None yet - needs systematic refactoring\n\n**Proposed Solution**:\n1. Create a configuration system\n2. Pass client_id as parameter through all functions\n3. Add client selection to CLI/UI\n\n**Workaround**:\nCurrently requires manual code changes for different clients\n\n---\n\n### [ISSUE-002] - No Error Recovery for API Failures\n**Status**: Open  \n**Severity**: High  \n**Type**: Bug  \n**Reported**: 2025-01-28  \n**Reporter**: Code Review  \n\n**Description**:\nOpenAI API calls have no retry logic or graceful degradation. System crashes if API is unavailable or rate limited.\n\n**Error Messages/Logs**:\n```\nopenai.error.RateLimitError: Rate limit exceeded\n```\n\n**Attempted Solutions**:\n1. None yet\n\n**Proposed Solution**:\n1. Implement exponential backoff retry logic\n2. Add fallback mechanisms for vendor classification\n3. Cache API responses to reduce calls\n\n**Workaround**:\nManual restart and hope for success\n\n---\n\n### [ISSUE-003] - Dashboard Performance Degradation\n**Status**: Open  \n**Severity**: Medium  \n**Type**: Performance  \n**Reported**: 2025-01-28  \n**Reporter**: Code Review  \n\n**Description**:\nStreamlit dashboards load slowly when dealing with large numbers of vendors (>1000). No pagination or lazy loading implemented.\n\n**Reproduction Steps**:\n1. Run mapping_review.py dashboard\n2. Load client with >1000 vendors\n3. Experience 10+ second load times\n\n**Attempted Solutions**:\n1. None yet\n\n**Proposed Solution**:\n1. Implement pagination\n2. Add caching for vendor data\n3. Optimize database queries\n4. Consider moving to more performant UI framework\n\n---\n\n### [ISSUE-004] - Duplicate Transaction Edge Cases\n**Status**: Open  \n**Severity**: Medium  \n**Type**: Bug  \n**Reported**: 2025-01-28  \n**Reporter**: Code Review  \n\n**Description**:\nDuplicate detection logic may fail for transactions with identical date, amount, and vendor but legitimate duplicates (e.g., multiple identical purchases same day).\n\n**Current Logic**:\nChecks date + vendor + amount + client for uniqueness\n\n**Attempted Solutions**:\n1. None yet\n\n**Proposed Solution**:\n1. Add transaction description to duplicate check\n2. Implement time-based windowing\n3. Add manual override option\n\n---\n\n### [ISSUE-005] - No Database Migration System\n**Status**: Open  \n**Severity**: Medium  \n**Type**: Design Flaw  \n**Reported**: 2025-01-29  \n**Reporter**: Documentation Review  \n\n**Description**:\nNo system for managing database schema changes. Makes updates risky and hard to track.\n\n**Proposed Solution**:\n1. Implement Alembic or similar migration tool\n2. Create initial migration from current schema\n3. Document migration procedures\n\n---\n\n## Resolved Issues\n\n### [ISSUE-000] - Example Resolved Issue\n**Status**: Resolved  \n**Severity**: Low  \n**Type**: Bug  \n**Reported**: 2025-01-01  \n**Reporter**: Example  \n**Resolved**: 2025-01-02  \n\n**Description**:\nExample of a resolved issue for reference.\n\n**Resolution**:\nFixed by updating configuration in commit abc123.\n\n---\n\n## Blocked Issues\n\n*None currently*\n\n---\n\n## Security Vulnerabilities\n\n### [SEC-001] - Potential SQL Injection in Raw Queries\n**Status**: Open  \n**Severity**: Critical  \n**Type**: Security  \n\n**Description**:\nSome database queries use string concatenation instead of parameterized queries.\n\n**Affected Files**:\n- Need full security audit to identify all instances\n\n**Proposed Solution**:\n1. Audit all database queries\n2. Replace with parameterized queries\n3. Add query validation layer\n\n---\n\n## Performance Bottlenecks\n\n### [PERF-001] - Sequential Vendor Processing\n**Status**: Open  \n**Severity**: Medium  \n**Type**: Performance  \n\n**Description**:\nVendors are processed one at a time in run_forecast.py, making large batches very slow.\n\n**Current Performance**:\n- ~2 seconds per vendor\n- 1000 vendors = ~33 minutes\n\n**Proposed Solution**:\n1. Implement parallel processing\n2. Batch API calls where possible\n3. Add progress reporting\n\n---\n\n## Known Limitations\n\n1. **Single data source**: Only Mercury CSV imports supported\n2. **No real-time updates**: Requires manual import/refresh\n3. **Limited to 13-week forecasts**: No long-term projections\n4. **No multi-currency support**: USD only\n5. **No mobile support**: Dashboards not responsive",
    "DECISIONS.md": "# Technical Decisions Log\n\n## Instructions\n- Document all significant technical decisions\n- Include reasoning, alternatives considered, and trade-offs\n- Link to relevant discussions or documentation\n- Update if decisions are reversed or modified\n\n## Decision Template\n```markdown\n### [DATE] - [Decision Title]\n**Status**: [Approved/Pending/Reversed]\n**Category**: [Architecture/Tools/Process/Security]\n**Decision**: [What was decided]\n**Reasoning**: [Why this decision was made]\n**Alternatives Considered**:\n1. [Alternative 1] - [Why rejected]\n2. [Alternative 2] - [Why rejected]\n**Trade-offs**:\n- Pros: [Benefits]\n- Cons: [Drawbacks]\n**Impact**: [How this affects the system]\n```\n\n## Decisions Made\n\n### 2025-01-29 - Documentation as Infrastructure\n**Status**: Approved  \n**Category**: Process  \n**Decision**: Implement comprehensive documentation system before any feature development  \n**Reasoning**: \n- Current codebase lacks proper documentation\n- Multiple files with unclear purposes\n- Need to maintain context across development sessions\n- Reduces onboarding time for new developers\n\n**Alternatives Considered**:\n1. Document as we go - Rejected: Often leads to incomplete documentation\n2. Document after refactoring - Rejected: Loses important context and decisions\n\n**Trade-offs**:\n- Pros: Better knowledge preservation, easier collaboration, reduced technical debt\n- Cons: Upfront time investment, requires discipline to maintain\n\n**Impact**: All development must update relevant documentation files\n\n---\n\n### 2025-01-28 - Supabase as Primary Database\n**Status**: Approved (Existing)  \n**Category**: Architecture  \n**Decision**: Use Supabase (PostgreSQL) as the primary database  \n**Reasoning**:\n- Provides real-time capabilities\n- Built-in authentication (for future use)\n- Managed PostgreSQL with good developer experience\n- RESTful API out of the box\n\n**Alternatives Considered**:\n1. Raw PostgreSQL - More control but more maintenance\n2. MongoDB - Better for unstructured data but less suitable for financial data\n3. SQLite - Too limited for multi-client architecture\n\n**Trade-offs**:\n- Pros: Managed service, built-in features, good scalability\n- Cons: Vendor lock-in, potential latency, cost at scale\n\n---\n\n### 2025-01-28 - OpenAI for Vendor Normalization\n**Status**: Approved (Existing)  \n**Category**: Architecture  \n**Decision**: Use OpenAI GPT-4 and embeddings for vendor name normalization  \n**Reasoning**:\n- Superior understanding of vendor name variations\n- Embeddings provide semantic similarity for clustering\n- Reduces manual mapping effort significantly\n\n**Alternatives Considered**:\n1. Rule-based system - Too rigid, high maintenance\n2. Fuzzy string matching - Limited accuracy for complex variations\n3. Open-source LLMs - Lower quality results for this use case\n\n**Trade-offs**:\n- Pros: High accuracy, low maintenance, handles edge cases well\n- Cons: API costs, dependency on external service, rate limits\n\n---\n\n### 2025-01-28 - Streamlit for Dashboards\n**Status**: Approved (Existing)  \n**Category**: Tools  \n**Decision**: Use Streamlit for all interactive dashboards  \n**Reasoning**:\n- Rapid development of data-centric UIs\n- Python-native (same language as backend)\n- Good for internal tools and MVPs\n\n**Alternatives Considered**:\n1. React/Next.js - Overkill for internal tools\n2. Flask/Django - More boilerplate for simple dashboards\n3. Dash - Less intuitive API than Streamlit\n\n**Trade-offs**:\n- Pros: Fast development, easy to maintain, good for data viz\n- Cons: Limited customization, not ideal for complex UIs, performance limitations\n\n---\n\n## Pending Decisions\n\n### Module Structure Reorganization\n**Status**: Pending  \n**Category**: Architecture  \n**Decision Needed**: How to organize the 40+ files into a proper module structure  \n**Options**:\n1. Domain-based (forecasting/, data/, ui/, etc.)\n2. Layer-based (models/, services/, controllers/, etc.)\n3. Feature-based (vendor_management/, forecasting/, reporting/, etc.)\n\n**Considerations**:\n- Current code coupling\n- Future scalability needs\n- Developer familiarity\n\n---\n\n### Testing Framework Selection\n**Status**: Pending  \n**Category**: Tools  \n**Decision Needed**: Which testing framework to use  \n**Options**:\n1. pytest - Most popular, good plugin ecosystem\n2. unittest - Built-in, familiar to Java developers\n3. nose2 - Good test discovery but less maintained\n\n---\n\n### Caching Strategy\n**Status**: Pending  \n**Category**: Architecture  \n**Decision Needed**: How to implement caching for API calls and computations  \n**Options**:\n1. Redis - Industry standard, good for distributed systems\n2. In-memory Python cache - Simpler but doesn't scale\n3. Supabase caching - Keep everything in one system\n\n---\n\n## Reversed Decisions\n\n*None yet*\n\n## Guidelines for Future Decisions\n\n1. **Prioritize maintainability** over clever solutions\n2. **Choose boring technology** when possible\n3. **Minimize external dependencies** for core functionality\n4. **Document extensively** for complex logic\n5. **Design for testability** from the start\n6. **Consider operational complexity** not just development ease"
  }
}